{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras.callbacks as cb\n",
    "import time\n",
    "import numpy as np\n",
    "from data import get_datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from functools import partial\n",
    "from data import get_datasets\n",
    "from experiment import create_model\n",
    "from sentiment import sentiment\n",
    "import model_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, test_dataset), info = get_datasets()\n",
    "encoder = info.features['text'].encoder\n",
    "vocab_size=info.features['text'].encoder.vocab_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously trained model.\n"
     ]
    }
   ],
   "source": [
    "transformer, _optimizer, _checkpoint, _manager = create_model(models_dir=\"models/pos_enc_True\", load_checkpoint=True, \n",
    "                           vocab_size=vocab_size, use_positional_encoding=True, run_eagerly=True)\n",
    "\n",
    "# adv = model_adv.create_model(models_dir=\"adv\", load_checkpoint=False, run_eagerly=True, d_model=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "D_MODEL = 128\n",
    "\n",
    "\n",
    "\n",
    "def find_adv_k(x, y, transformer):\n",
    "    \"\"\"\n",
    "    x - (batch, text)\n",
    "    y - (batch, label)\n",
    "    \n",
    "    Passes x through transformer and receives y_logits, w, k.\n",
    "    Creates an adversarial model - adv.\n",
    "    Pass k through adv model and receives adv_k.\n",
    "    Computes loss by passing (x, custom_k=adv_k) through transformer:\n",
    "        Receives adv_y_logits, adv_w, adv_k (same k)\n",
    "        loss_t = tf.keras.losses.binary_crossentropy(y_true=y, y_pred=adv_y_logits, from_logits=True)\n",
    "        loss_k = - |k - adv_k|\n",
    "        loss = loss_t + loss_k\n",
    "    \n",
    "    Returns: \n",
    "        k - original k\n",
    "        w - original attention weights\n",
    "        adv_k - adversarial k\n",
    "        adv_w - adversarial attention weights w\n",
    "        loss_k - the one that is maximized\n",
    "        loss_t - transformer loss (the one that is minimized)\n",
    "        loss - adversarial loss, e.g.  loss = loss_t = loss_k\n",
    "    \"\"\"\n",
    "    best_loss_t = None\n",
    "    best_adv_k = None\n",
    "    \n",
    "\n",
    "    y_logits, w, k = transformer(x, training=False)\n",
    "    print(f'Returned k: {tf.shape(k)}')\n",
    "    y_logits2, _, _ = transformer(x, training=False, custom_k=k)\n",
    "    \n",
    "    adv, optimizer = model_adv.create_model(models_dir=\"adv\", load_checkpoint=False, \n",
    "                                            run_eagerly=True, d_model=D_MODEL)\n",
    "    \n",
    "    loss_t = tf.keras.losses.binary_crossentropy(y_true=y, y_pred=y_logits, from_logits=True)\n",
    "    best_loss_t = loss_t\n",
    "    original_loss_t = loss_t\n",
    "    original_k = k\n",
    "    \n",
    "    print(f'[{0}] Loss_t = {loss_t:<20.10}')\n",
    "    loss_t = tf.keras.losses.binary_crossentropy(y_true=y, y_pred=y_logits2, from_logits=True)\n",
    "    print(f'[{0}] Loss_t (custom k) = {loss_t:<20.10}')\n",
    "    \n",
    "    alpha = 0.9999\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):        \n",
    "        with tf.GradientTape() as tape:\n",
    "            adv_k = adv(k, training=True)\n",
    "            if best_adv_k is None:\n",
    "                best_adv_k = adv_k\n",
    "            \n",
    "            adv_y_logits, adv_w, adv_k2  = transformer(x, custom_k=adv_k, training=False)\n",
    "            cond = tf.reduce_all(tf.equal(adv_k, adv_k2)).numpy()\n",
    "            bs = tf.shape(k)[0]\n",
    "            assert cond == True\n",
    "            loss_t = tf.keras.losses.binary_crossentropy(y_true=y, y_pred=adv_y_logits, from_logits=True)\n",
    "            summed = tf.math.reduce_sum(tf.math.pow(tf.reshape(k, [bs, -1]) - tf.reshape(adv_k, [bs, -1]), 2), axis=1)\n",
    "            \n",
    "            loss_k = - tf.math.reduce_mean(summed)\n",
    "            loss = alpha*loss_t + (1.0 - alpha)*loss_k\n",
    "            \n",
    "            if loss_t < best_loss_t:\n",
    "                best_loss_t = loss_t\n",
    "                best_adv_k  = adv_k\n",
    "            \n",
    "        \n",
    "        print(f'[{epoch:<2}] Loss_t = {loss_t:<20.10}  Loss_k = {loss_k:<20.10}   Loss = {loss:<20.10}')\n",
    "\n",
    "        grads = tape.gradient(loss, adv.trainable_weights)                \n",
    "        optimizer.apply_gradients(zip(grads, adv.trainable_weights))\n",
    "  \n",
    "    return original_loss_t, original_k, best_loss_t, best_adv_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned k: [ 64   4 194 128]\n",
      "[0] Loss_t = 0.1850163043        \n",
      "[0] Loss_t (custom k) = 0.1850163043        \n",
      "[1 ] Loss_t = 0.4331596494          Loss_k = -150493.1719           Loss = -14.61620045        \n",
      "[2 ] Loss_t = 0.3474534154          Loss_k = -173734.3594           Loss = -17.02601814        \n",
      "[3 ] Loss_t = 0.291121304           Loss_k = -200938.8594           Loss = -19.8027935         \n",
      "[4 ] Loss_t = 0.2605164051          Loss_k = -232728.6406           Loss = -23.01237297        \n",
      "[5 ] Loss_t = 0.2482067347          Loss_k = -269986.3125           Loss = -26.75044823        \n",
      "[6 ] Loss_t = 0.2455919236          Loss_k = -313791.125            Loss = -31.13354492        \n",
      "[7 ] Loss_t = 0.2470159978          Loss_k = -365251.5              Loss = -36.2781601         \n",
      "[8 ] Loss_t = 0.2497117817          Loss_k = -425645.5625           Loss = -42.31486893        \n",
      "[9 ] Loss_t = 0.2523645759          Loss_k = -496556.5625           Loss = -49.4033165         \n",
      "[10] Loss_t = 0.2542693615          Loss_k = -579769.875            Loss = -57.72274017        \n",
      "[11] Loss_t = 0.2553200126          Loss_k = -677309.0625           Loss = -67.47560883        \n",
      "[12] Loss_t = 0.2555637062          Loss_k = -791592.5              Loss = -78.90370941        \n",
      "[13] Loss_t = 0.2551598847          Loss_k = -925352.5              Loss = -92.28011322        \n",
      "[14] Loss_t = 0.2541828752          Loss_k = -1081638.0             Loss = -107.9096375        \n",
      "[15] Loss_t = 0.2526983619          Loss_k = -1263872.5             Loss = -126.1345749        \n",
      "[16] Loss_t = 0.250992775           Loss_k = -1475941.25            Loss = -147.3431549        \n",
      "[17] Loss_t = 0.2491336465          Loss_k = -1722265.75            Loss = -171.9774628        \n",
      "[18] Loss_t = 0.247321263           Loss_k = -2007874.75            Loss = -200.5401764        \n",
      "[19] Loss_t = 0.2457987368          Loss_k = -2338416.75            Loss = -233.5959015        \n",
      "[20] Loss_t = 0.2446504831          Loss_k = -2720075.0             Loss = -271.7628784        \n",
      "[21] Loss_t = 0.2441208512          Loss_k = -3159781.0             Loss = -315.7339783        \n",
      "[22] Loss_t = 0.244470641           Loss_k = -3665289.0             Loss = -366.2844543        \n",
      "[23] Loss_t = 0.2460719049          Loss_k = -4245260.0             Loss = -424.2799683        \n",
      "[24] Loss_t = 0.2492160648          Loss_k = -4909292.5             Loss = -490.6800537        \n",
      "[25] Loss_t = 0.2539983094          Loss_k = -5668099.0             Loss = -566.5559082        \n",
      "[26] Loss_t = 0.2604200542          Loss_k = -6533670.0             Loss = -653.1066284        \n",
      "[27] Loss_t = 0.268397063           Loss_k = -7519064.0             Loss = -751.6380005        \n",
      "[28] Loss_t = 0.2780433893          Loss_k = -8638788.0             Loss = -863.600769         \n",
      "[29] Loss_t = 0.2892599106          Loss_k = -9909084.0             Loss = -990.6191406        \n",
      "[30] Loss_t = 0.301841706           Loss_k = -11348401.0            Loss = -1134.53833         \n",
      "[31] Loss_t = 0.3157094717          Loss_k = -12976741.0            Loss = -1297.358398        \n",
      "[32] Loss_t = 0.3308915794          Loss_k = -14816214.0            Loss = -1481.290527        \n",
      "[33] Loss_t = 0.3473012745          Loss_k = -16891372.0            Loss = -1688.789917        \n",
      "[34] Loss_t = 0.3649824262          Loss_k = -19228488.0            Loss = -1922.483765        \n",
      "[35] Loss_t = 0.3837555647          Loss_k = -21856448.0            Loss = -2185.260986        \n",
      "[36] Loss_t = 0.4037599862          Loss_k = -24807312.0            Loss = -2480.327393        \n",
      "[37] Loss_t = 0.4252356291          Loss_k = -28115542.0            Loss = -2811.128906        \n",
      "[38] Loss_t = 0.4481571019          Loss_k = -31818852.0            Loss = -3181.437012        \n",
      "[39] Loss_t = 0.4721704125          Loss_k = -35959340.0            Loss = -3595.46167         \n",
      "[40] Loss_t = 0.4972987473          Loss_k = -40583192.0            Loss = -4057.821777        \n",
      "[41] Loss_t = 0.5233331323          Loss_k = -45739116.0            Loss = -4573.388184        \n",
      "[42] Loss_t = 0.5502087474          Loss_k = -51480220.0            Loss = -5147.47168         \n",
      "[43] Loss_t = 0.5780316591          Loss_k = -57863468.0            Loss = -5785.768555        \n",
      "[44] Loss_t = 0.6068658233          Loss_k = -64951332.0            Loss = -6494.525879        \n",
      "[45] Loss_t = 0.6365257502          Loss_k = -72811808.0            Loss = -7280.544434        \n",
      "[46] Loss_t = 0.6666074395          Loss_k = -81519888.0            Loss = -8151.322266        \n",
      "[47] Loss_t = 0.6970703602          Loss_k = -91154584.0            Loss = -9114.760742        \n",
      "[48] Loss_t = 0.7278362513          Loss_k = -101800624.0           Loss = -10179.33496        \n",
      "[49] Loss_t = 0.7584148645          Loss_k = -113549328.0           Loss = -11354.17383        \n",
      "[50] Loss_t = 0.7887220383          Loss_k = -126494408.0           Loss = -12648.65137        \n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_dataset))\n",
    "#     for batch, (x, y) in enumerate(train_dataset):\n",
    "loss_t, k, best_loss_t, best_adv_k = find_adv_k(x, y, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.1850163, shape=(), dtype=float32)\n",
      "tf.Tensor(0.1850163, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-5.01886308e-02 -1.64749131e-01  6.57088682e-02  1.29200399e+00\n",
      " -6.17367089e-01  4.79863137e-02  2.02779993e-01  2.81210303e-01\n",
      "  1.17436022e-01  4.75367457e-01 -1.51617639e-03  4.78382438e-01\n",
      " -1.07432950e+00 -1.10027768e-01  1.01997055e-01 -6.40636742e-01\n",
      " -3.84948522e-01 -1.79203898e-01  2.02490919e-04  9.92396951e-01\n",
      "  9.80666950e-02 -1.79248899e-01 -2.01024994e-01 -1.04747105e+00\n",
      " -1.81850165e-01 -1.59880206e-01 -7.11462200e-01  4.23706174e-01\n",
      "  5.38996398e-01  3.63422751e-01  4.64513123e-01  3.26554865e-01\n",
      " -3.49301249e-01 -6.68285728e-01  1.50979722e+00 -7.32147843e-02\n",
      "  9.85800754e-04 -2.60735989e-01 -3.84469897e-01  3.37077171e-01\n",
      "  8.64960849e-01  6.10059023e-01  1.07984984e+00  2.57375509e-01\n",
      "  9.72188890e-01  1.05649281e+00  1.32399893e+00  2.93195367e-01\n",
      " -5.40404499e-01  5.35042524e-01  1.11359501e+00  9.66471851e-01\n",
      "  8.72436166e-01 -1.49107361e+00  1.22011757e+00 -5.92127860e-01\n",
      "  2.79199183e-01 -2.81281501e-01  2.25440666e-01 -6.12088084e-01\n",
      " -5.38552642e-01 -1.55532300e+00  1.35249496e+00  1.24769711e+00\n",
      " -1.34342742e+00 -4.70031828e-01 -1.10152755e-02  7.33362734e-01\n",
      " -4.36631292e-01  2.69137293e-01 -1.07375824e+00 -5.12148321e-01\n",
      " -7.47503877e-01 -3.93730551e-01  7.42387891e-01 -1.98900789e-01\n",
      "  5.56458175e-01 -1.10566914e-01 -4.59260494e-01  2.42921859e-01\n",
      " -5.25485694e-01 -9.13136065e-01  1.57880142e-01  8.39032173e-01\n",
      " -9.24081922e-01  5.06736875e-01 -6.33763254e-01 -6.89327493e-02\n",
      "  1.25834489e+00 -6.23227179e-01  1.08338702e+00  1.00736983e-01\n",
      "  1.08807981e+00 -7.77153432e-01 -2.14068437e+00 -2.37184223e-02\n",
      " -1.97153851e-01  1.50396991e+00  2.89404929e-01  1.80937489e-03\n",
      " -4.53559369e-01 -1.41654527e+00 -5.16487837e-01  1.02559447e+00\n",
      " -4.61303629e-02 -3.04541111e-01  8.21714401e-01  1.88311052e+00\n",
      "  7.80942738e-01 -2.12830499e-01 -2.45542020e-01  2.77608097e-01\n",
      " -2.60243297e-01 -1.77662671e+00 -1.59342133e-03  1.30661285e+00\n",
      " -3.66067380e-01 -3.05603389e-02  6.51506424e-01  3.16151410e-01\n",
      " -4.25577492e-01  2.67225262e-02 -8.16549957e-01  9.74969566e-02\n",
      " -8.27282965e-01 -6.82810843e-01 -9.59904134e-01 -1.15501165e+00], shape=(128,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[ 0.1428853  -0.62310416 -1.2080486  -0.29645714 -0.04558251  0.3002698\n",
      " -0.00444782  0.06045574 -0.5402711   0.7845241   0.35926583 -0.27692842\n",
      " -0.04873774  0.1996876   0.16850103 -0.00669028 -0.64425355  0.8579403\n",
      "  0.1817984   0.09912664 -0.14584777 -0.9763306   0.4415637   0.42911947\n",
      "  0.24273953 -0.27381375  0.43594646  0.18727256  0.37327462 -0.3365467\n",
      " -0.6766834   0.08854301  0.6098646  -0.07878666  0.90035594 -0.2579662\n",
      "  1.019158   -0.03611632 -0.44977704  0.43747246 -0.24374875  0.49531588\n",
      " -0.08608256  0.36119428  0.7291048  -0.22715437  0.03057745  0.32133284\n",
      " -0.5713443  -0.29200363  0.4877958  -0.52823347  0.3487264   0.41182706\n",
      " -0.6591198   0.0144854  -0.04228867 -0.21205775  0.15276562 -0.02436826\n",
      " -0.10842457  0.01674835 -0.8703792  -0.32104567  0.10701285 -0.06289109\n",
      "  0.14823988  0.974465    0.6758108  -0.22186291  0.20971304  0.32200843\n",
      " -0.50554997  0.11472104 -0.3179005   0.20897439  0.34713113  0.54365003\n",
      " -0.26160234  0.42408094 -0.33756322  0.8636374  -0.03618064 -0.06336231\n",
      "  0.2790926   0.11802076  0.6062119  -0.29504028 -0.6294706  -0.80004776\n",
      " -0.09354323  0.4776134  -0.18383832 -0.41645786 -0.40456444  0.25817394\n",
      "  0.10586026 -0.17492825  0.65802217  0.7270353  -0.5197629   0.87403363\n",
      "  0.34344283 -0.42570645 -0.24237056 -0.7271955   0.56543267  0.1395045\n",
      " -0.24735071 -0.7765396  -0.18766043 -0.20337617 -0.1376489  -0.24409048\n",
      "  0.20278862 -0.53907585 -0.10750347  0.38668165 -0.90782714  0.6806368\n",
      " -0.0501842  -0.33296546  0.6192195   0.06623383  0.340689    1.3056241\n",
      " -0.16197565  0.00753066], shape=(128,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(loss_t)\n",
    "print(best_loss_t)\n",
    "print(k[0][0][0])\n",
    "print(best_adv_k[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('best_adv_k', best_adv_k.numpy())\n",
    "np.save('k', k.numpy())\n",
    "np.save('loss_t', loss_t.numpy())\n",
    "np.save('best_loss_t', best_loss_t.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1850163\n",
      "0.1850163\n"
     ]
    }
   ],
   "source": [
    "# Load the best values\n",
    "\n",
    "np.load('best_adv_k.npy')\n",
    "np.load('k.npy')\n",
    "print(np.load('loss_t.npy'))\n",
    "print(np.load('best_loss_t.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
