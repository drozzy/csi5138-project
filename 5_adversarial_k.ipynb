{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras.callbacks as cb\n",
    "import time\n",
    "import numpy as np\n",
    "from data import get_datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from functools import partial\n",
    "from data import get_datasets\n",
    "from experiment import create_model\n",
    "from sentiment import sentiment\n",
    "import model_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, test_dataset), info = get_datasets()\n",
    "encoder = info.features['text'].encoder\n",
    "vocab_size=info.features['text'].encoder.vocab_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously trained model.\n"
     ]
    }
   ],
   "source": [
    "transformer, _optimizer, _checkpoint, _manager = create_model(models_dir=\"models/pos_enc_True\", load_checkpoint=True, \n",
    "                           vocab_size=vocab_size, use_positional_encoding=True, run_eagerly=True)\n",
    "\n",
    "# adv = model_adv.create_model(models_dir=\"adv\", load_checkpoint=False, run_eagerly=True, d_model=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "D_MODEL = 128\n",
    "def find_adv_k(x, y, transformer):\n",
    "    \"\"\"\n",
    "    x - (batch, text)\n",
    "    y - (batch, label)\n",
    "    \n",
    "    Passes x through transformer and receives y_logits, w, k.\n",
    "    Creates an adversarial model - adv.\n",
    "    Pass k through adv model and receives adv_k.\n",
    "    Computes loss by passing (x, custom_k=adv_k) through transformer:\n",
    "        Receives adv_y_logits, adv_w, adv_k (same k)\n",
    "        loss_t = tf.keras.losses.binary_crossentropy(y_true=y, y_pred=adv_y_logits, from_logits=True)\n",
    "        loss_k = - |k - adv_k|\n",
    "        loss = loss_t + loss_k\n",
    "    \n",
    "    Returns: \n",
    "        k - original k\n",
    "        w - original attention weights\n",
    "        adv_k - adversarial k\n",
    "        adv_w - adversarial attention weights w\n",
    "        loss_k - the one that is maximized\n",
    "        loss_t - transformer loss (the one that is minimized)\n",
    "        loss - adversarial loss, e.g.  loss = loss_t = loss_k\n",
    "    \"\"\"\n",
    "    y_logits, w, k = transformer(x, training=False)\n",
    "    adv, optimizer = model_adv.create_model(models_dir=\"adv\", load_checkpoint=False, \n",
    "                                            run_eagerly=True, d_model=D_MODEL)\n",
    "    \n",
    "    loss_t = tf.keras.losses.binary_crossentropy(y_true=y, y_pred=y_logits, from_logits=True)\n",
    "    print(f'[{0}] Loss_t = {loss_t:<20.10}')\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):        \n",
    "        with tf.GradientTape() as tape:\n",
    "            adv_k = adv(k, training=True)\n",
    "            \n",
    "            adv_y_logits, adv_w, adv_k2  = transformer(x, custom_k=adv_k, training=False)\n",
    "            cond = tf.reduce_all(tf.equal(adv_k, adv_k2)).numpy()\n",
    "            \n",
    "            assert cond == True\n",
    "            loss_t = tf.keras.losses.binary_crossentropy(y_true=y, y_pred=adv_y_logits, from_logits=True)\n",
    "            loss_k = - tf.math.reduce_sum(tf.math.abs(k - adv_k))\n",
    "            loss = loss_t + loss_k\n",
    "        \n",
    "        print(f'[{epoch:<2}] Loss_t = {loss_t:<20.10}  Loss_k = {loss_k:<20.10}   Loss = {loss:<20.10}')\n",
    "\n",
    "        grads = tape.gradient(loss, adv.trainable_weights)                \n",
    "        optimizer.apply_gradients(zip(grads, adv.trainable_weights))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Loss_t = 0.2084553838        \n",
      "[1 ] Loss_t = 0.5221247673          Loss_k = -1806262.5             Loss = -1806262.0          \n",
      "[2 ] Loss_t = 0.5203827024          Loss_k = -2095850.625           Loss = -2095850.125        \n",
      "[3 ] Loss_t = 0.5209338665          Loss_k = -2410787.0             Loss = -2410786.5          \n",
      "[4 ] Loss_t = 0.5219099522          Loss_k = -2755802.5             Loss = -2755802.0          \n",
      "[5 ] Loss_t = 0.5249029994          Loss_k = -3135517.25            Loss = -3135516.75         \n",
      "[6 ] Loss_t = 0.527751863           Loss_k = -3554072.75            Loss = -3554072.25         \n",
      "[7 ] Loss_t = 0.5303804278          Loss_k = -4016218.0             Loss = -4016217.5          \n",
      "[8 ] Loss_t = 0.5345689058          Loss_k = -4525277.5             Loss = -4525277.0          \n",
      "[9 ] Loss_t = 0.538839817           Loss_k = -5084765.5             Loss = -5084765.0          \n",
      "[10] Loss_t = 0.5418853164          Loss_k = -5697141.0             Loss = -5697140.5          \n",
      "[11] Loss_t = 0.5433930159          Loss_k = -6364614.0             Loss = -6364613.5          \n",
      "[12] Loss_t = 0.5444076061          Loss_k = -7089345.0             Loss = -7089344.5          \n",
      "[13] Loss_t = 0.5453671217          Loss_k = -7873369.5             Loss = -7873369.0          \n",
      "[14] Loss_t = 0.5464115739          Loss_k = -8718266.0             Loss = -8718265.0          \n",
      "[15] Loss_t = 0.5476993322          Loss_k = -9626032.0             Loss = -9626031.0          \n",
      "[16] Loss_t = 0.5494315624          Loss_k = -10598634.0            Loss = -10598633.0         \n",
      "[17] Loss_t = 0.5517200828          Loss_k = -11638264.0            Loss = -11638263.0         \n",
      "[18] Loss_t = 0.5544947386          Loss_k = -12747012.0            Loss = -12747011.0         \n",
      "[19] Loss_t = 0.5575442314          Loss_k = -13926942.0            Loss = -13926941.0         \n",
      "[20] Loss_t = 0.5600436926          Loss_k = -15180126.0            Loss = -15180125.0         \n",
      "[21] Loss_t = 0.5616887808          Loss_k = -16508597.0            Loss = -16508596.0         \n",
      "[22] Loss_t = 0.5632715225          Loss_k = -17914262.0            Loss = -17914262.0         \n",
      "[23] Loss_t = 0.5652176142          Loss_k = -19399162.0            Loss = -19399162.0         \n",
      "[24] Loss_t = 0.567550838           Loss_k = -20965190.0            Loss = -20965190.0         \n",
      "[25] Loss_t = 0.5701304078          Loss_k = -22614200.0            Loss = -22614200.0         \n",
      "[26] Loss_t = 0.5729789734          Loss_k = -24348060.0            Loss = -24348060.0         \n",
      "[27] Loss_t = 0.5761321187          Loss_k = -26168628.0            Loss = -26168628.0         \n",
      "[28] Loss_t = 0.5796217322          Loss_k = -28077734.0            Loss = -28077734.0         \n",
      "[29] Loss_t = 0.5834798217          Loss_k = -30077168.0            Loss = -30077168.0         \n",
      "[30] Loss_t = 0.5877401233          Loss_k = -32168692.0            Loss = -32168692.0         \n",
      "[31] Loss_t = 0.5924472809          Loss_k = -34354032.0            Loss = -34354032.0         \n",
      "[32] Loss_t = 0.5976501703          Loss_k = -36634852.0            Loss = -36634852.0         \n",
      "[33] Loss_t = 0.6033933163          Loss_k = -39012808.0            Loss = -39012808.0         \n",
      "[34] Loss_t = 0.6097217202          Loss_k = -41489476.0            Loss = -41489476.0         \n",
      "[35] Loss_t = 0.6166958809          Loss_k = -44066420.0            Loss = -44066420.0         \n",
      "[36] Loss_t = 0.6243743896          Loss_k = -46745156.0            Loss = -46745156.0         \n",
      "[37] Loss_t = 0.6328248978          Loss_k = -49527128.0            Loss = -49527128.0         \n",
      "[38] Loss_t = 0.6421071291          Loss_k = -52413792.0            Loss = -52413792.0         \n",
      "[39] Loss_t = 0.652287662           Loss_k = -55406516.0            Loss = -55406516.0         \n",
      "[40] Loss_t = 0.6634407043          Loss_k = -58506652.0            Loss = -58506652.0         \n",
      "[41] Loss_t = 0.6756447554          Loss_k = -61715480.0            Loss = -61715480.0         \n",
      "[42] Loss_t = 0.6889573932          Loss_k = -65034276.0            Loss = -65034276.0         \n",
      "[43] Loss_t = 0.7034509182          Loss_k = -68464248.0            Loss = -68464248.0         \n",
      "[44] Loss_t = 0.7191611528          Loss_k = -72006560.0            Loss = -72006560.0         \n",
      "[45] Loss_t = 0.7361488342          Loss_k = -75662368.0            Loss = -75662368.0         \n",
      "[46] Loss_t = 0.7544526458          Loss_k = -79432800.0            Loss = -79432800.0         \n",
      "[47] Loss_t = 0.7740921378          Loss_k = -83318848.0            Loss = -83318848.0         \n",
      "[48] Loss_t = 0.7950717211          Loss_k = -87321600.0            Loss = -87321600.0         \n",
      "[49] Loss_t = 0.8173832297          Loss_k = -91442016.0            Loss = -91442016.0         \n",
      "[50] Loss_t = 0.8409728408          Loss_k = -95681088.0            Loss = -95681088.0         \n",
      "[51] Loss_t = 0.8657675385          Loss_k = -100039696.0           Loss = -100039696.0        \n",
      "[52] Loss_t = 0.8916841745          Loss_k = -104518760.0           Loss = -104518760.0        \n",
      "[53] Loss_t = 0.9186025858          Loss_k = -109119120.0           Loss = -109119120.0        \n",
      "[54] Loss_t = 0.9463985562          Loss_k = -113841624.0           Loss = -113841624.0        \n",
      "[55] Loss_t = 0.9749565721          Loss_k = -118687064.0           Loss = -118687064.0        \n",
      "[56] Loss_t = 1.004146576           Loss_k = -123656200.0           Loss = -123656200.0        \n",
      "[57] Loss_t = 1.033852339           Loss_k = -128749808.0           Loss = -128749808.0        \n",
      "[58] Loss_t = 1.063930511           Loss_k = -133968552.0           Loss = -133968552.0        \n",
      "[59] Loss_t = 1.094282985           Loss_k = -139313152.0           Loss = -139313152.0        \n",
      "[60] Loss_t = 1.124754667           Loss_k = -144784272.0           Loss = -144784272.0        \n",
      "[61] Loss_t = 1.155240417           Loss_k = -150382528.0           Loss = -150382528.0        \n",
      "[62] Loss_t = 1.185631514           Loss_k = -156108576.0           Loss = -156108576.0        \n",
      "[63] Loss_t = 1.215843678           Loss_k = -161962960.0           Loss = -161962960.0        \n",
      "[64] Loss_t = 1.245735407           Loss_k = -167946256.0           Loss = -167946256.0        \n",
      "[65] Loss_t = 1.275221109           Loss_k = -174059056.0           Loss = -174059056.0        \n",
      "[66] Loss_t = 1.30420351            Loss_k = -180301824.0           Loss = -180301824.0        \n",
      "[67] Loss_t = 1.332606435           Loss_k = -186675152.0           Loss = -186675152.0        \n",
      "[68] Loss_t = 1.360340595           Loss_k = -193179424.0           Loss = -193179424.0        \n",
      "[69] Loss_t = 1.387329578           Loss_k = -199815168.0           Loss = -199815168.0        \n",
      "[70] Loss_t = 1.413542747           Loss_k = -206582816.0           Loss = -206582816.0        \n",
      "[71] Loss_t = 1.438885689           Loss_k = -213482832.0           Loss = -213482832.0        \n",
      "[72] Loss_t = 1.463323712           Loss_k = -220515584.0           Loss = -220515584.0        \n",
      "[73] Loss_t = 1.48682642            Loss_k = -227681520.0           Loss = -227681520.0        \n",
      "[74] Loss_t = 1.509308577           Loss_k = -234980976.0           Loss = -234980976.0        \n",
      "[75] Loss_t = 1.530743122           Loss_k = -242414336.0           Loss = -242414336.0        \n",
      "[76] Loss_t = 1.551113844           Loss_k = -249981968.0           Loss = -249981968.0        \n",
      "[77] Loss_t = 1.57040751            Loss_k = -257684208.0           Loss = -257684208.0        \n",
      "[78] Loss_t = 1.588598967           Loss_k = -265521376.0           Loss = -265521376.0        \n",
      "[79] Loss_t = 1.605662584           Loss_k = -273493792.0           Loss = -273493792.0        \n",
      "[80] Loss_t = 1.62156558            Loss_k = -281601760.0           Loss = -281601760.0        \n",
      "[81] Loss_t = 1.636304617           Loss_k = -289845536.0           Loss = -289845536.0        \n",
      "[82] Loss_t = 1.649912119           Loss_k = -298225472.0           Loss = -298225472.0        \n",
      "[83] Loss_t = 1.662371278           Loss_k = -306741760.0           Loss = -306741760.0        \n",
      "[84] Loss_t = 1.673658848           Loss_k = -315394656.0           Loss = -315394656.0        \n",
      "[85] Loss_t = 1.683806896           Loss_k = -324184448.0           Loss = -324184448.0        \n",
      "[86] Loss_t = 1.692806721           Loss_k = -333111392.0           Loss = -333111392.0        \n",
      "[87] Loss_t = 1.70066452            Loss_k = -342175584.0           Loss = -342175584.0        \n",
      "[88] Loss_t = 1.707386494           Loss_k = -351377408.0           Loss = -351377408.0        \n",
      "[89] Loss_t = 1.713042974           Loss_k = -360716928.0           Loss = -360716928.0        \n",
      "[90] Loss_t = 1.717631102           Loss_k = -370194432.0           Loss = -370194432.0        \n",
      "[91] Loss_t = 1.721187949           Loss_k = -379810048.0           Loss = -379810048.0        \n",
      "[92] Loss_t = 1.723750949           Loss_k = -389564064.0           Loss = -389564064.0        \n",
      "[93] Loss_t = 1.72552073            Loss_k = -399456448.0           Loss = -399456448.0        \n",
      "[94] Loss_t = 1.72649157            Loss_k = -409487584.0           Loss = -409487584.0        \n",
      "[95] Loss_t = 1.726592064           Loss_k = -419657472.0           Loss = -419657472.0        \n",
      "[96] Loss_t = 1.725840211           Loss_k = -429966368.0           Loss = -429966368.0        \n",
      "[97] Loss_t = 1.72426641            Loss_k = -440414368.0           Loss = -440414368.0        \n",
      "[98] Loss_t = 1.721894979           Loss_k = -451001632.0           Loss = -451001632.0        \n",
      "[99] Loss_t = 1.718781948           Loss_k = -461728224.0           Loss = -461728224.0        \n",
      "[100] Loss_t = 1.714996934           Loss_k = -472594400.0           Loss = -472594400.0        \n",
      "[0] Loss_t = 0.204678908         \n",
      "[1 ] Loss_t = 0.6421602964          Loss_k = -1832118.75            Loss = -1832118.125        \n",
      "[2 ] Loss_t = 0.6385618448          Loss_k = -2129750.5             Loss = -2129749.75         \n",
      "[3 ] Loss_t = 0.6309784651          Loss_k = -2452687.5             Loss = -2452686.75         \n",
      "[4 ] Loss_t = 0.6204534769          Loss_k = -2809333.0             Loss = -2809332.5          \n",
      "[5 ] Loss_t = 0.6129955649          Loss_k = -3205397.75            Loss = -3205397.25         \n",
      "[6 ] Loss_t = 0.6105256081          Loss_k = -3643460.0             Loss = -3643459.5          \n",
      "[7 ] Loss_t = 0.608423233           Loss_k = -4125300.25            Loss = -4125299.75         \n",
      "[8 ] Loss_t = 0.6062991619          Loss_k = -4653976.0             Loss = -4653975.5          \n",
      "[9 ] Loss_t = 0.6054809093          Loss_k = -5232370.0             Loss = -5232369.5          \n",
      "[10] Loss_t = 0.6050269008          Loss_k = -5863445.5             Loss = -5863445.0          \n",
      "[11] Loss_t = 0.604635179           Loss_k = -6549711.0             Loss = -6549710.5          \n",
      "[12] Loss_t = 0.6041218042          Loss_k = -7293213.5             Loss = -7293213.0          \n",
      "[13] Loss_t = 0.6033556461          Loss_k = -8095979.0             Loss = -8095978.5          \n",
      "[14] Loss_t = 0.6023033261          Loss_k = -8959896.0             Loss = -8959895.0          \n",
      "[15] Loss_t = 0.6010011435          Loss_k = -9886982.0             Loss = -9886981.0          \n",
      "[16] Loss_t = 0.5995400548          Loss_k = -10879418.0            Loss = -10879417.0         \n",
      "[17] Loss_t = 0.5979859829          Loss_k = -11939515.0            Loss = -11939514.0         \n",
      "[18] Loss_t = 0.5963213444          Loss_k = -13069396.0            Loss = -13069395.0         \n",
      "[19] Loss_t = 0.5947133303          Loss_k = -14271076.0            Loss = -14271075.0         \n",
      "[20] Loss_t = 0.5930233002          Loss_k = -15546508.0            Loss = -15546507.0         \n",
      "[21] Loss_t = 0.5918103456          Loss_k = -16897752.0            Loss = -16897752.0         \n",
      "[22] Loss_t = 0.5909627676          Loss_k = -18326682.0            Loss = -18326682.0         \n",
      "[23] Loss_t = 0.5901267529          Loss_k = -19835288.0            Loss = -19835288.0         \n",
      "[24] Loss_t = 0.5894048214          Loss_k = -21425502.0            Loss = -21425502.0         \n",
      "[25] Loss_t = 0.5887300968          Loss_k = -23099208.0            Loss = -23099208.0         \n",
      "[26] Loss_t = 0.5881146193          Loss_k = -24858292.0            Loss = -24858292.0         \n",
      "[27] Loss_t = 0.5875681639          Loss_k = -26704588.0            Loss = -26704588.0         \n",
      "[28] Loss_t = 0.5870994925          Loss_k = -28639896.0            Loss = -28639896.0         \n",
      "[29] Loss_t = 0.5867282152          Loss_k = -30665974.0            Loss = -30665974.0         \n",
      "[30] Loss_t = 0.5864696503          Loss_k = -32784532.0            Loss = -32784532.0         \n",
      "[31] Loss_t = 0.5863406658          Loss_k = -34997256.0            Loss = -34997256.0         \n",
      "[32] Loss_t = 0.5863540173          Loss_k = -37305760.0            Loss = -37305760.0         \n",
      "[33] Loss_t = 0.5865353346          Loss_k = -39711648.0            Loss = -39711648.0         \n",
      "[34] Loss_t = 0.5869013071          Loss_k = -42216472.0            Loss = -42216472.0         \n",
      "[35] Loss_t = 0.5874744654          Loss_k = -44821748.0            Loss = -44821748.0         \n",
      "[36] Loss_t = 0.5882814527          Loss_k = -47528940.0            Loss = -47528940.0         \n",
      "[37] Loss_t = 0.5893435478          Loss_k = -50339488.0            Loss = -50339488.0         \n",
      "[38] Loss_t = 0.5906844735          Loss_k = -53254784.0            Loss = -53254784.0         \n",
      "[39] Loss_t = 0.592328012           Loss_k = -56276184.0            Loss = -56276184.0         \n",
      "[40] Loss_t = 0.5942984819          Loss_k = -59405008.0            Loss = -59405008.0         \n",
      "[41] Loss_t = 0.5966258049          Loss_k = -62642528.0            Loss = -62642528.0         \n",
      "[42] Loss_t = 0.5993317366          Loss_k = -65989972.0            Loss = -65989972.0         \n",
      "[43] Loss_t = 0.6024349928          Loss_k = -69448544.0            Loss = -69448544.0         \n",
      "[44] Loss_t = 0.6059452891          Loss_k = -73019424.0            Loss = -73019424.0         \n",
      "[45] Loss_t = 0.6098816991          Loss_k = -76703704.0            Loss = -76703704.0         \n",
      "[46] Loss_t = 0.614250958           Loss_k = -80502496.0            Loss = -80502496.0         \n",
      "[47] Loss_t = 0.6190683842          Loss_k = -84416832.0            Loss = -84416832.0         \n",
      "[48] Loss_t = 0.6243314743          Loss_k = -88447744.0            Loss = -88447744.0         \n",
      "[49] Loss_t = 0.6300451756          Loss_k = -92596224.0            Loss = -92596224.0         \n",
      "[50] Loss_t = 0.6362169981          Loss_k = -96863208.0            Loss = -96863208.0         \n",
      "[51] Loss_t = 0.642855823           Loss_k = -101249616.0           Loss = -101249616.0        \n",
      "[52] Loss_t = 0.6499596238          Loss_k = -105756344.0           Loss = -105756344.0        \n",
      "[53] Loss_t = 0.6575605869          Loss_k = -110384248.0           Loss = -110384248.0        \n",
      "[54] Loss_t = 0.6656589508          Loss_k = -115134112.0           Loss = -115134112.0        \n",
      "[55] Loss_t = 0.6742275953          Loss_k = -120006800.0           Loss = -120006800.0        \n",
      "[56] Loss_t = 0.6832816005          Loss_k = -125003040.0           Loss = -125003040.0        \n",
      "[57] Loss_t = 0.6928079128          Loss_k = -130123576.0           Loss = -130123576.0        \n",
      "[58] Loss_t = 0.7027906179          Loss_k = -135369120.0           Loss = -135369120.0        \n",
      "[59] Loss_t = 0.7131865621          Loss_k = -140740352.0           Loss = -140740352.0        \n",
      "[60] Loss_t = 0.7239762545          Loss_k = -146237952.0           Loss = -146237952.0        \n",
      "[61] Loss_t = 0.7351068258          Loss_k = -151862592.0           Loss = -151862592.0        \n",
      "[62] Loss_t = 0.7465206385          Loss_k = -157614800.0           Loss = -157614800.0        \n",
      "[63] Loss_t = 0.758167088           Loss_k = -163495248.0           Loss = -163495248.0        \n",
      "[64] Loss_t = 0.7699981928          Loss_k = -169504496.0           Loss = -169504496.0        \n",
      "[65] Loss_t = 0.7819439173          Loss_k = -175643040.0           Loss = -175643040.0        \n",
      "[66] Loss_t = 0.7939378619          Loss_k = -181911456.0           Loss = -181911456.0        \n",
      "[67] Loss_t = 0.8059450388          Loss_k = -188310256.0           Loss = -188310256.0        \n",
      "[68] Loss_t = 0.8178990483          Loss_k = -194839872.0           Loss = -194839872.0        \n",
      "[69] Loss_t = 0.8297562599          Loss_k = -201500848.0           Loss = -201500848.0        \n",
      "[70] Loss_t = 0.8414486647          Loss_k = -208293584.0           Loss = -208293584.0        \n",
      "[71] Loss_t = 0.8529341221          Loss_k = -215218576.0           Loss = -215218576.0        \n",
      "[72] Loss_t = 0.8641790748          Loss_k = -222276160.0           Loss = -222276160.0        \n",
      "[73] Loss_t = 0.8751747608          Loss_k = -229466832.0           Loss = -229466832.0        \n",
      "[74] Loss_t = 0.8859115839          Loss_k = -236790880.0           Loss = -236790880.0        \n",
      "[75] Loss_t = 0.8963588476          Loss_k = -244248768.0           Loss = -244248768.0        \n",
      "[76] Loss_t = 0.9065089226          Loss_k = -251840800.0           Loss = -251840800.0        \n",
      "[77] Loss_t = 0.9163337946          Loss_k = -259567328.0           Loss = -259567328.0        \n",
      "[78] Loss_t = 0.9258250594          Loss_k = -267428672.0           Loss = -267428672.0        \n",
      "[79] Loss_t = 0.934961617           Loss_k = -275425184.0           Loss = -275425184.0        \n",
      "[80] Loss_t = 0.9437452555          Loss_k = -283557184.0           Loss = -283557184.0        \n",
      "[81] Loss_t = 0.9521734715          Loss_k = -291824896.0           Loss = -291824896.0        \n",
      "[82] Loss_t = 0.960238874           Loss_k = -300228608.0           Loss = -300228608.0        \n",
      "[83] Loss_t = 0.9679555893          Loss_k = -308768640.0           Loss = -308768640.0        \n",
      "[84] Loss_t = 0.975320518           Loss_k = -317445248.0           Loss = -317445248.0        \n",
      "[85] Loss_t = 0.9823566079          Loss_k = -326258688.0           Loss = -326258688.0        \n",
      "[86] Loss_t = 0.9890804291          Loss_k = -335209120.0           Loss = -335209120.0        \n",
      "[87] Loss_t = 0.9954972267          Loss_k = -344296864.0           Loss = -344296864.0        \n",
      "[88] Loss_t = 1.001605988           Loss_k = -353522080.0           Loss = -353522080.0        \n",
      "[89] Loss_t = 1.007417798           Loss_k = -362884992.0           Loss = -362884992.0        \n",
      "[90] Loss_t = 1.012934804           Loss_k = -372385856.0           Loss = -372385856.0        \n",
      "[91] Loss_t = 1.018185019           Loss_k = -382024768.0           Loss = -382024768.0        \n",
      "[92] Loss_t = 1.023168802           Loss_k = -391801984.0           Loss = -391801984.0        \n",
      "[93] Loss_t = 1.027876616           Loss_k = -401717632.0           Loss = -401717632.0        \n",
      "[94] Loss_t = 1.03230536            Loss_k = -411771904.0           Loss = -411771904.0        \n",
      "[95] Loss_t = 1.036471128           Loss_k = -421965024.0           Loss = -421965024.0        \n",
      "[96] Loss_t = 1.04037571            Loss_k = -432297024.0           Loss = -432297024.0        \n",
      "[97] Loss_t = 1.04403913            Loss_k = -442768160.0           Loss = -442768160.0        \n",
      "[98] Loss_t = 1.047446132           Loss_k = -453378496.0           Loss = -453378496.0        \n",
      "[99] Loss_t = 1.050638795           Loss_k = -464128192.0           Loss = -464128192.0        \n",
      "[100] Loss_t = 1.05357337            Loss_k = -475017408.0           Loss = -475017408.0        \n",
      "[0] Loss_t = 0.1798231304        \n",
      "[1 ] Loss_t = 0.597427547           Loss_k = -1861233.75            Loss = -1861233.125        \n",
      "[2 ] Loss_t = 0.5991823673          Loss_k = -2174271.75            Loss = -2174271.25         \n",
      "[3 ] Loss_t = 0.6034836769          Loss_k = -2504794.75            Loss = -2504794.25         \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e975b9da770e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mfind_adv_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-f01bb4b61907>\u001b[0m in \u001b[0;36mfind_adv_k\u001b[1;34m(x, y, transformer)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'[{epoch:<2}] Loss_t = {loss_t:<20.10}  Loss_k = {loss_k:<20.10}   Loss = {loss:<20.10}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_SquaredDifferenceGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1461\u001b[0m     \u001b[1;31m# The parens ensure that if grad is IndexedSlices, it'll get multiplied by\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1462\u001b[0m     \u001b[1;31m# Tensor (not a number like 2.0) which causes it to convert to Tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1463\u001b[1;33m     \u001b[0mx_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m   if (isinstance(grad, ops.Tensor) and\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mscalar_mul\u001b[1;34m(scalar, x, name)\u001b[0m\n\u001b[0;32m    420\u001b[0m           gen_math_ops.mul(scalar, x.values, name), x.indices, x.dense_shape)\n\u001b[0;32m    421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Only scalar multiply works, got shape %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6683\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   6684\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Mul\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6685\u001b[1;33m         name, _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[0;32m   6686\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6687\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch, (x, y) in enumerate(train_dataset):\n",
    "    find_adv_k(x, y, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.628025472164154\n",
    "w2 = 0.1232"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6280254722         0.1232              \n",
      "0.1232               0.6280254722        \n"
     ]
    }
   ],
   "source": [
    "print(f'{w:<20.10} {w2:<20.10}')\n",
    "print(f'{w2:<20.10} {w:<20.10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
