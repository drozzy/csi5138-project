{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras.callbacks as cb\n",
    "import time\n",
    "import numpy as np\n",
    "from data import get_datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from functools import partial\n",
    "from data import get_datasets\n",
    "from experiment import create_model\n",
    "from sentiment import sentiment\n",
    "import model_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, test_dataset), info = get_datasets(batch_size=1)\n",
    "encoder = info.features['text'].encoder\n",
    "vocab_size=info.features['text'].encoder.vocab_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previously trained model.\n"
     ]
    }
   ],
   "source": [
    "transformer, _optimizer, _checkpoint, _manager = create_model(models_dir=\"models/pos_enc_True\", load_checkpoint=True, \n",
    "                           vocab_size=vocab_size, use_positional_encoding=True, run_eagerly=True)\n",
    "\n",
    "# adv = model_adv.create_model(models_dir=\"adv\", load_checkpoint=False, run_eagerly=True, d_model=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "D_MODEL = 128\n",
    "\n",
    "def find_adv_k(x, y, transformer):\n",
    "    \"\"\"\n",
    "    x - (batch, text)\n",
    "    y - (batch, label)\n",
    "    \n",
    "    Passes x through transformer and receives y_logits, w, k.\n",
    "    Creates an adversarial model - adv.\n",
    "    Pass k through adv model and receives adv_k.\n",
    "    Computes loss by passing (x, custom_k=adv_k) through transformer:\n",
    "        Receives adv_y_logits, adv_w, adv_k (same k)\n",
    "        loss_t = tf.keras.losses.binary_crossentropy(y_true=y, y_pred=adv_y_logits, from_logits=True)\n",
    "        loss_k = - |k - adv_k|\n",
    "        loss = loss_t + loss_k\n",
    "    \n",
    "    Returns: \n",
    "        k - original k\n",
    "        w - original attention weights\n",
    "        adv_k - adversarial k\n",
    "        adv_w - adversarial attention weights w\n",
    "        loss_k - the one that is maximized\n",
    "        loss_t - transformer loss (the one that is minimized)\n",
    "        loss - adversarial loss, e.g.  loss = loss_t = loss_k\n",
    "    \"\"\"\n",
    "    best_loss_t = None\n",
    "    best_adv_k = None\n",
    "    \n",
    "\n",
    "    y_logits, w, k = transformer(x, training=False)\n",
    "    print(f'Returned k: {tf.shape(k)}')\n",
    "    y_logits2, _, _ = transformer(x, training=False, custom_k=k)\n",
    "    \n",
    "    adv, optimizer = model_adv.create_model(models_dir=\"adv\", load_checkpoint=False, \n",
    "                                            run_eagerly=True, d_model=D_MODEL)\n",
    "    \n",
    "    loss_t = tf.keras.losses.binary_crossentropy(y_true=y, y_pred=y_logits, from_logits=True)\n",
    "    best_loss_t = loss_t\n",
    "    original_loss_t = loss_t\n",
    "    original_k = k\n",
    "    \n",
    "    print(f'[{0}] Loss_t = {loss_t:<20.10}')\n",
    "    loss_t = tf.keras.losses.binary_crossentropy(y_true=y, y_pred=y_logits2, from_logits=True)\n",
    "    print(f'[{0}] Loss_t (custom k) = {loss_t:<20.10}')\n",
    "    \n",
    "    alpha = 0.9999\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, EPOCHS+1):        \n",
    "        with tf.GradientTape() as tape:\n",
    "            adv_k = adv(k, training=True)\n",
    "            if best_adv_k is None:\n",
    "                best_adv_k = adv_k\n",
    "            \n",
    "            adv_y_logits, adv_w, adv_k2  = transformer(x, custom_k=adv_k, training=False)\n",
    "            cond = tf.reduce_all(tf.equal(adv_k, adv_k2)).numpy()\n",
    "            bs = tf.shape(k)[0]\n",
    "            assert cond == True\n",
    "            loss_t = tf.keras.losses.binary_crossentropy(y_true=y, y_pred=adv_y_logits, from_logits=True)\n",
    "            summed = tf.math.reduce_sum(tf.math.pow(tf.reshape(k, [bs, -1]) - tf.reshape(adv_k, [bs, -1]), 2), axis=1)\n",
    "            \n",
    "            loss_k = - tf.math.reduce_mean(summed)\n",
    "            loss = alpha*loss_t + (1.0 - alpha)*loss_k\n",
    "            \n",
    "            if loss_t < best_loss_t:\n",
    "                best_loss_t = loss_t\n",
    "                best_adv_k  = adv_k\n",
    "            \n",
    "        \n",
    "        print(f'[{epoch:<2}] Loss_t = {loss_t:<20.10}  Loss_k = {loss_k:<20.10}   Loss = {loss:<20.10}')\n",
    "\n",
    "        grads = tape.gradient(loss, adv.trainable_weights)                \n",
    "        optimizer.apply_gradients(zip(grads, adv.trainable_weights))\n",
    "  \n",
    "    return original_loss_t, original_k, best_loss_t, best_adv_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned k: [  1   4  56 128]\n",
      "[0] Loss_t = 0.02403318696       \n",
      "[0] Loss_t (custom k) = 0.02403318696       \n",
      "[1 ] Loss_t = 0.05705162138         Loss_k = -43891.63281           Loss = -4.332117081        \n",
      "[2 ] Loss_t = 0.05273097008         Loss_k = -51835.21094           Loss = -5.130795002        \n",
      "[3 ] Loss_t = 0.04886035249         Loss_k = -61121.37109           Loss = -6.063281536        \n",
      "[4 ] Loss_t = 0.04534782469         Loss_k = -71994.59375           Loss = -7.154115677        \n",
      "[5 ] Loss_t = 0.04206982255         Loss_k = -84694.39062           Loss = -8.427372932        \n",
      "[6 ] Loss_t = 0.03912955523         Loss_k = -99606.90625           Loss = -9.921565056        \n",
      "[7 ] Loss_t = 0.03650836274         Loss_k = -117143.4219           Loss = -11.67783737        \n",
      "[8 ] Loss_t = 0.03418093175         Loss_k = -137766.4531           Loss = -13.74246693        \n",
      "[9 ] Loss_t = 0.03208424896         Loss_k = -162000.8125           Loss = -16.16799927        \n",
      "[10] Loss_t = 0.03020050563         Loss_k = -190462.9062           Loss = -19.0160923         \n",
      "[11] Loss_t = 0.02850283124         Loss_k = -223851.6406           Loss = -22.35666466        \n",
      "[12] Loss_t = 0.02696029656         Loss_k = -262972.875            Loss = -26.27032852        \n",
      "[13] Loss_t = 0.02555342391         Loss_k = -308763.3125           Loss = -30.85078049        \n",
      "[14] Loss_t = 0.02425465547         Loss_k = -362257.6562           Loss = -36.20151138        \n",
      "[15] Loss_t = 0.02309122868         Loss_k = -424613.5              Loss = -42.43825912        \n",
      "[16] Loss_t = 0.02201749384         Loss_k = -497156.4062           Loss = -49.6936264         \n",
      "[17] Loss_t = 0.02108656801         Loss_k = -581430.25             Loss = -58.12194061        \n",
      "[18] Loss_t = 0.02023007907         Loss_k = -679162.375            Loss = -67.89601135        \n",
      "[19] Loss_t = 0.0194519069          Loss_k = -792267.0              Loss = -79.2072525         \n",
      "[20] Loss_t = 0.01873350888         Loss_k = -922873.0625           Loss = -92.26856995        \n",
      "[21] Loss_t = 0.0180526711          Loss_k = -1073425.625           Loss = -107.3245087        \n",
      "[22] Loss_t = 0.01740765199         Loss_k = -1246730.5             Loss = -124.6556473        \n",
      "[23] Loss_t = 0.01676446386         Loss_k = -1445888.5             Loss = -144.5720825        \n",
      "[24] Loss_t = 0.0161327105          Loss_k = -1674264.25            Loss = -167.4102936        \n",
      "[25] Loss_t = 0.01553399954         Loss_k = -1935718.125           Loss = -193.5562744        \n",
      "[26] Loss_t = 0.0150010623          Loss_k = -2234618.75            Loss = -223.4468689        \n",
      "[27] Loss_t = 0.01454350073         Loss_k = -2575527.5             Loss = -257.5381775        \n",
      "[28] Loss_t = 0.01416120399         Loss_k = -2963720.75            Loss = -296.3579102        \n",
      "[29] Loss_t = 0.01385736465         Loss_k = -3405053.0             Loss = -340.4914246        \n",
      "[30] Loss_t = 0.01362156682         Loss_k = -3905991.0             Loss = -390.5854797        \n",
      "[31] Loss_t = 0.01346299797         Loss_k = -4473887.5             Loss = -447.3752747        \n",
      "[32] Loss_t = 0.01337825693         Loss_k = -5116860.0             Loss = -511.6726074        \n",
      "[33] Loss_t = 0.0133733796          Loss_k = -5843467.0             Loss = -584.333313         \n",
      "[34] Loss_t = 0.01343908813         Loss_k = -6663216.0             Loss = -666.3081665        \n",
      "[35] Loss_t = 0.01359070744         Loss_k = -7586135.5             Loss = -758.5999146        \n",
      "[36] Loss_t = 0.01384546608         Loss_k = -8623633.0             Loss = -862.3494263        \n",
      "[37] Loss_t = 0.01420950703         Loss_k = -9788372.0             Loss = -978.822937         \n",
      "[38] Loss_t = 0.01470352896         Loss_k = -11094072.0            Loss = -1109.392578        \n",
      "[39] Loss_t = 0.01535889506         Loss_k = -12555253.0            Loss = -1255.509888        \n",
      "[40] Loss_t = 0.01619514264         Loss_k = -14187633.0            Loss = -1418.74707         \n",
      "[41] Loss_t = 0.01728818007         Loss_k = -16008907.0            Loss = -1600.873291        \n",
      "[42] Loss_t = 0.01864686608         Loss_k = -18039002.0            Loss = -1803.88147         \n",
      "[43] Loss_t = 0.02036151104         Loss_k = -20297914.0            Loss = -2029.770996        \n",
      "[44] Loss_t = 0.02259445749         Loss_k = -22807952.0            Loss = -2280.772461        \n",
      "[45] Loss_t = 0.02546119317         Loss_k = -25593076.0            Loss = -2559.282227        \n",
      "[46] Loss_t = 0.02930065431         Loss_k = -28679694.0            Loss = -2867.939941        \n",
      "[47] Loss_t = 0.03442497551         Loss_k = -32094514.0            Loss = -3209.416992        \n",
      "[48] Loss_t = 0.04137601703         Loss_k = -35868392.0            Loss = -3586.797852        \n",
      "[49] Loss_t = 0.05126783997         Loss_k = -40031728.0            Loss = -4003.121338        \n",
      "[50] Loss_t = 0.06539244205         Loss_k = -44616492.0            Loss = -4461.583496        \n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_dataset))\n",
    "#     for batch, (x, y) in enumerate(train_dataset):\n",
    "loss_t, k, best_loss_t, best_adv_k = find_adv_k(x, y, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss_t: 0.013373379595577717\n",
      "Original loss_t: 0.024033186957240105\n"
     ]
    }
   ],
   "source": [
    "print(f'Best loss_t: {best_loss_t}')\n",
    "print(f'Original loss_t: {loss_t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.10514279 -0.7446283  -0.37535304  0.7732584  -0.1891772   0.7325174\n",
      " -0.05518687  1.0767896   0.11997275  0.49861163 -0.09280941 -0.0515768\n",
      " -0.61197674  0.60941297 -0.20162156 -1.0039016  -1.4525076  -0.34856468\n",
      "  0.31456915  0.7075647  -1.091167   -0.68078005 -0.67013174 -0.96027386\n",
      " -0.7915867  -0.23067713 -1.0558378   0.13707896  0.7173154   0.00786834\n",
      "  0.85752827  0.90897775 -0.31805348  0.44462976  0.3518911   0.15995233\n",
      " -0.21541786  0.5320765  -0.3091937   0.23128088  0.84781516  0.89916176\n",
      " -0.2199289   0.0950876   0.13624406  1.1994206   0.43694854  0.5668871\n",
      " -0.8313954   0.1284687   0.8594588   0.70675844  0.3895667  -0.21708894\n",
      "  1.3379259  -0.90666705 -0.6052241   0.17854702 -0.89288545 -0.02649421\n",
      " -0.70923495 -1.9466168   0.58874416  0.4724625  -1.5841348  -0.2232738\n",
      " -0.17609075  0.48799652 -0.96991855  1.281586   -1.060228   -0.46949005\n",
      " -0.13753739 -0.37512478  1.5053993  -0.01799305 -0.10419203  0.20251174\n",
      " -1.3018364   0.28232265  0.716445   -0.60712516  0.19047101  0.9115541\n",
      " -0.20347847  1.3546005  -0.16230473 -0.36449802  1.416369   -0.02344655\n",
      "  0.12085804  1.3425996   0.4031997  -1.7768013  -0.71834785  0.05356997\n",
      " -0.7143315   1.8746758   0.19838603 -0.824118    0.20828697 -0.8602154\n",
      " -1.1906691   0.6317738  -0.81749505  1.1792766   1.0259281   0.9701676\n",
      " -0.09945016 -1.2715274  -0.6412849   0.43905473 -0.7951206  -0.39604303\n",
      " -0.03077762  1.5845546  -0.70266783 -0.66052806  0.42099065 -0.05103882\n",
      "  0.24794653 -0.4344648  -1.0738975  -0.18878457 -0.2705106   0.4740847\n",
      " -0.5349683  -1.5068034 ], shape=(128,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-5.879565   -3.0974786  -0.41564757 -2.9487805   3.5470233   0.45264572\n",
      "  2.5467868   3.3851812  -3.3660657  -2.3102977   3.6368349  -2.6010358\n",
      "  0.7072221  -1.3516016   1.0997611   3.0343523  -3.4311373   3.4605868\n",
      "  3.042592   -4.558849    3.1612842  -2.7898352   2.400801   -3.6496496\n",
      " -4.842462   -4.6196294  -0.36552328  4.437758   -2.7530773   4.1141863\n",
      " -2.7583342   5.0768933   2.743592    0.9601297   0.41253695  2.6045122\n",
      " -1.8563122   3.0432181   3.2086992   3.5116835   1.4516208   1.4028364\n",
      " -2.6617627   2.6750624  -2.7762196  -3.6689086  -3.9167552   0.20135526\n",
      " -1.6105396  -0.26116133 -3.2323558  -3.4134617   1.1678786   1.3764836\n",
      " -2.857739    2.5341897  -5.145879    2.449       2.8060446  -2.1818693\n",
      "  0.7926574   2.2113361  -0.60126555 -1.0424639  -3.8177385  -4.4597206\n",
      " -4.0297694  -3.948595    2.2471056   3.0514796   3.9471993  -3.0016503\n",
      "  1.9281036  -1.1265209  -2.125878   -4.053472    4.10912     1.3522102\n",
      "  3.199286   -3.0663836  -1.1524884  -1.49714    -1.2380799  -4.776257\n",
      "  1.74658     3.5192597   2.6868687   1.7820832  -3.3151727  -0.16706891\n",
      "  2.8947318  -2.149473   -4.7037864   2.7376947   2.4317026   3.9305568\n",
      " -3.0987527  -1.8241595   3.621252   -3.701054    4.8233285   2.7990594\n",
      "  1.7540435  -3.2278678  -2.442623    1.5961514   2.2671885   1.8277301\n",
      "  1.3819017   4.311308    1.715855   -3.1593575   2.7137063  -2.3502936\n",
      " -3.8626313   3.320191    3.1318207  -1.8013377  -2.464092   -1.819392\n",
      " -2.8881216  -0.35361063  2.8678627   4.1492014   4.3442726   0.19361392\n",
      "  3.337958   -1.5192767 ], shape=(128,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(k[0][0][0])\n",
    "print(best_adv_k[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('adv_results/x', x.numpy())\n",
    "np.save('adv_results/y', y.numpy())\n",
    "np.save('adv_results/best_adv_k', best_adv_k.numpy())\n",
    "np.save('adv_results/k', k.numpy())\n",
    "np.save('adv_results/loss_t', loss_t.numpy())\n",
    "np.save('adv_results/best_loss_t', best_loss_t.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024033187\n",
      "0.01337338\n"
     ]
    }
   ],
   "source": [
    "# Load the best values\n",
    "np.load('adv_results/x.npy')\n",
    "np.load('adv_results/y.npy')\n",
    "np.load('adv_results/best_adv_k.npy')\n",
    "np.load('adv_results/k.npy')\n",
    "print(np.load('adv_results/loss_t.npy'))\n",
    "print(np.load('adv_results/best_loss_t.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
