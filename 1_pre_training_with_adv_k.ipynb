{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JjJJyJTZYebt"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras.callbacks as cb\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from functools import partial\n",
    "from model import TransformerEncoderClassifier\n",
    "from experiment import create_model, fit_data\n",
    "from data import get_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 200 # Can test with smaller datasets, 200 default!\n",
    "EPOCHS = 10     # Can run for less epochs, 100 default!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, test_dataset), info = get_datasets(max_length=MAX_LENGTH)\n",
    "\n",
    "encoder = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiysUa--4tOU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not loading previously trained model. Training from scratch.\n"
     ]
    }
   ],
   "source": [
    "vocab_size=info.features['text'].encoder.vocab_size \n",
    "transformer, optimizer, checkpoint, manager = create_model(models_dir=\"models/pos_enc_True\", load_checkpoint=False, \n",
    "                           vocab_size=vocab_size, use_positional_encoding=True, run_eagerly=True)\n",
    "results_dir=\"results\"\n",
    "models_dir=\"models\"\n",
    "data_dir=\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Test Accuracy=0.5157037973403931\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1209 15:40:57.250759  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:01.054759  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:04.722265  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:08.447729  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:12.092227  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:15.806228  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:19.709261  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:23.547229  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:27.293759  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:31.055253  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:41:34.724259  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:38.432258  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:42.081761  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:45.798259  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:49.454228  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:53.191759  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:41:56.917261  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:00.545762  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:04.212759  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:07.943260  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:42:11.702256  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:15.465760  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:19.199758  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:22.943759  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:26.665761  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:30.523730  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:34.336233  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:38.105258  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:41.827259  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:45.538760  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:42:49.255760  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:52.983748  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:42:56.680756  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:00.354754  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:04.085729  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:07.747261  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:11.393258  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:15.047259  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:18.729259  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:22.468259  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:43:26.170727  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:30.095762  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:33.832759  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:37.632338  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:41.373259  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:45.152730  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:48.999759  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:52.728758  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:43:56.484330  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:00.222823  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:44:03.937323  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:07.609824  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:11.331323  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:15.039322  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:18.725324  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:22.418325  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:26.092323  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:29.777824  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:33.490323  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:37.208822  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:44:40.978827  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:44.669825  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:48.399322  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:52.141323  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:55.846167  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:44:59.649668  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:03.419671  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:07.298168  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:11.171147  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:14.938668  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:45:18.712670  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:22.400168  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:26.055165  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:29.719167  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:33.354177  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:37.143673  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:40.938667  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:44.748142  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:48.517168  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:52.252680  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:45:56.080167  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:45:59.880667  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:03.659643  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:07.423177  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:11.223669  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:15.019142  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:18.737667  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:22.398170  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:26.045667  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:29.876673  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:46:33.690668  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:37.461169  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:41.128671  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:44.809668  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:48.534170  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:52.219668  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:55.903668  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:46:59.540167  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:03.165169  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:06.817670  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:47:10.449167  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:14.111668  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:17.792682  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:21.490668  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:25.297167  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:29.044165  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:32.769165  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:36.487669  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:40.197668  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:43.886167  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:47:47.570169  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:51.359671  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:55.103574  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:47:58.825075  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:02.626540  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:06.344571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:10.048072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:13.756072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:17.497045  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:21.261075  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:48:24.898571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:28.612072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:32.380576  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:36.119073  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:39.807073  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:43.527072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:47.255570  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:50.931572  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:54.641067  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:48:58.350571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:49:02.060071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:49:05.770076  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:49:09.570071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:49:11.153071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Train Loss: 29.459360122680664\n",
      "\t Test Accuracy=0.48429620265960693\n",
      "\t Saved checkpoint better accuracy for epoch 1: models/pos_enc_True\\ckpt-1\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:49:28.671572  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:49:32.387071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:49:36.081572  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:49:39.780571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:49:43.491155  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:49:47.220571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:49:50.936540  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:49:54.715543  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:49:58.466540  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:02.170571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:50:05.836571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:09.588571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:13.384571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:17.141040  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:20.875071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:24.584071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:28.268073  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:31.923072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:35.699074  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:39.542546  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:50:43.274071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:46.982543  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:50.686071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:54.494071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:50:58.197066  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:01.960066  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:05.696542  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:09.473574  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:13.165040  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:16.800071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:51:20.478540  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:24.082573  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:27.761072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:31.449575  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:35.121573  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:38.832074  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:42.477040  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:46.135574  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:49.799571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:51:53.414071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:51:57.059575  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:00.699571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:04.352571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:08.029071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:11.637580  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:15.338071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:19.032572  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:22.663571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:26.332573  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:29.993571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:52:33.683043  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:37.399039  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:41.090571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:44.759575  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:48.431074  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:52.113072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:55.796073  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:52:59.465072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:03.146571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:06.833071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:53:10.513541  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:14.180071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:17.862071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:21.532572  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:25.191074  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:28.875574  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:32.562071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:36.245069  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:39.896072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:43.564072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:53:47.259072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:50.932571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:54.594042  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:53:58.282574  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:01.961039  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:05.622573  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:09.279571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:12.954071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:16.614571  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:20.287073  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:54:24.009075  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:27.780572  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:31.666572  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:35.598072  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:39.310073  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:43.017573  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:46.732074  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:50.432071  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:54.205691  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:54:57.943691  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:55:01.585690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:05.283191  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:08.933691  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:12.651691  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:16.354691  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:20.044690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:23.772191  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:27.471690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:31.190690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:34.900690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:55:38.704690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:42.495191  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:46.276690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:49.981690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:53.684190  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:55:57.440690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:01.132191  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:04.825690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:08.507690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:12.240690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:56:15.923191  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:19.571690  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:23.241691  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:26.903746  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:30.630749  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:34.292246  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:37.986746  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:41.634245  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:45.291745  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:48.994248  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:56:52.691747  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:56:56.329044  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:57:00.137042  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:57:03.954541  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:57:07.705044  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:57:11.535538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:57:15.331544  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:57:19.019013  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:57:22.792540  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:57:26.529038  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:57:30.224040  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:57:33.911542  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:57:37.591538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:57:39.130013  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Train Loss: 5.225003719329834\n",
      "\t Test Accuracy=0.48429620265960693\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:57:56.552038  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:00.238039  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:03.924042  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:07.577538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:11.280038  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:14.919039  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:18.598539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:22.207541  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:25.943041  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:29.731039  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:58:33.374538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:37.082539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:40.709038  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:44.420515  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:48.203542  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:51.916538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:55.604045  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:58:59.210539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:02.854539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:06.515515  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:59:10.152538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:13.849040  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:17.579044  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:21.313552  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:25.022538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:28.688038  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:32.338541  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:36.023539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:39.687539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:43.332539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 15:59:47.005539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:50.667541  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:54.352543  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 15:59:58.088041  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:01.732039  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:05.378041  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:09.041539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:12.658538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:16.281040  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:19.913040  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:00:23.581040  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:27.238534  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:30.908539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:34.558512  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:38.277038  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:41.940544  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:45.739048  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:49.427539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:53.131539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:00:56.795538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:01:00.443040  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:04.169538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:07.815538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:11.490539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:15.232539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:18.915039  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:22.544039  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:26.237048  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:29.961538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:33.644538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:01:37.307538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:40.942039  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:44.607041  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:48.272538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:51.938539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:55.658039  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:01:59.322539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:03.003046  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:06.633538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:10.312539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:02:13.973539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:17.569540  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:21.243040  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:24.887538  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:28.522543  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:32.270516  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:36.011039  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:39.681539  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:43.541391  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:47.370886  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:02:51.081386  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:54.799887  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:02:58.583386  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:02.723387  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:06.423386  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:10.169886  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:13.917886  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:17.719886  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:21.460887  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:25.173387  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:03:28.851386  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:32.564386  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:36.293386  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:39.992886  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:43.677887  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:47.389887  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:51.057788  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:54.766290  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:03:58.498288  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:02.360788  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:04:06.083788  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:09.799291  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:13.498791  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:17.177291  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:20.917788  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:24.636256  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:28.282290  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:31.982790  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:35.766913  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:39.498412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:04:43.181414  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:46.894912  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:50.630412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:54.312414  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:04:57.987912  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:01.732915  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:05.445913  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:09.165913  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:12.908412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:16.734412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:05:20.443413  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:24.113916  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:27.789912  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:31.451412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:35.210412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:38.893412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:42.556414  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:46.202912  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:49.904416  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:05:53.610924  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:05:57.272916  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:01.021413  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:04.722913  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:06.263387  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Train Loss: 3.7825937271118164\n",
      "\t Test Accuracy=0.48429620265960693\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:06:23.446391  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:27.116890  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:30.837412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:34.477412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:38.183917  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:41.829913  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:45.465914  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:49.098912  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:52.734917  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:06:56.352412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:06:59.993912  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:03.597413  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:07.219414  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:10.828914  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:14.463918  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:18.086913  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:21.723914  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:25.364416  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:29.021917  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:32.649917  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:07:36.313412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:40.005412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:43.661414  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:47.325915  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:50.962913  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:54.626413  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:07:58.302912  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:01.965912  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:05.627889  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:09.320413  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:08:13.008914  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:16.731914  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:20.417915  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:24.074917  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:27.752912  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:31.442412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:35.192916  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:38.984412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:42.710413  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:46.471918  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 16:08:50.274413  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:53.889412  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:08:57.595919  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:09:01.353914  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:09:05.078912  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n",
      "W1209 16:09:08.752915  4396 optimizer_v2.py:1029] Gradients do not exist for variables ['transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer/multi_head_attention/dense_1/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_1/multi_head_attention_1/dense_7/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_2/multi_head_attention_2/dense_13/bias:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/kernel:0', 'transformer_encoder_classifier/encoder/encoder_layer_3/multi_head_attention_3/dense_19/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3245cc6b3ddb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_with_adv_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Projects\\csi5138-project\\experiment.py\u001b[0m in \u001b[0;36mfit_data\u001b[1;34m(max_epochs, model, optimizer, checkpoint, manager, train_dataset, test_dataset, results_dir, models_dir, train_with_adv_k)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_with_adv_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mtest_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\csi5138-project\\experiment.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(epoch, model, optimizer, train_dataset, metrics, train_with_adv_k)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrain_with_adv_k\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[0mtrain_batch_with_k_adv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\csi5138-project\\experiment.py\u001b[0m in \u001b[0;36mtrain_batch_with_k_adv\u001b[1;34m(epoch, batch, x, y, model, optimizer, metrics)\u001b[0m\n\u001b[0;32m    231\u001b[0m                                             run_eagerly=True, d_model=D_MODEL)  \n\u001b[0;32m    232\u001b[0m     loss_t, k, best_loss_t, best_adv_k = find_adv_k(x, y, model,\n\u001b[1;32m--> 233\u001b[1;33m                 adv_model, adv_optimizer, epochs=20,random_k=True)\n\u001b[0m\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\csi5138-project\\experiment.py\u001b[0m in \u001b[0;36mfind_adv_k\u001b[1;34m(x, y, transformer, adv_model, adv_optimizer, epochs, random_k)\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mbest_adv_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madv_k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0madv_y_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv_k2\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madv_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[0mcond\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madv_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv_k2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\csi5138-project\\model.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inp, training, custom_k)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0menc_padding_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_padding_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m         \u001b[0menc_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_padding_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_k\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m         \u001b[1;31m# enc_output.shape == (batch_size, seq_len, d_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0menc_padding_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\csi5138-project\\model.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, training, mask, custom_k)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_k\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcustom_k\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m             \u001b[0mks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[0mattention_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\csi5138-project\\model.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x, training, mask, custom_k)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mffn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mffn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mout2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayernorm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mffn_output\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, input_seq_len, d_model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m         \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m         variance_epsilon=self.epsilon)\n\u001b[0m\u001b[0;32m   1023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m     \u001b[1;31m# If some components of the shape got lost due to adjustments, fix that.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py\u001b[0m in \u001b[0;36mbatch_normalization\u001b[1;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[0;32m   1434\u001b[0m   \"\"\"\n\u001b[0;32m   1435\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"batchnorm\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m     \u001b[0minv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvariance_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1437\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m       \u001b[0minv\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    910\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1195\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1197\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1198\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    531\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m    532\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"AddV2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         name, _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[0;32m    534\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_history = fit_data(20, transformer, optimizer, checkpoint, manager, train_dataset, test_dataset, results_dir, models_dir,train_with_adv_k = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40625\n",
      "0.4375\n",
      "0.421875\n",
      "0.421875\n",
      "0.446875\n",
      "0.42447916\n",
      "0.4486607\n",
      "0.4609375\n",
      "0.4704861\n",
      "0.475\n",
      "0.47585228\n",
      "0.47395834\n",
      "0.47355768\n",
      "0.4765625\n",
      "0.48125\n",
      "0.48046875\n",
      "0.4788603\n",
      "0.48350695\n",
      "0.4819079\n",
      "0.4796875\n",
      "0.47767857\n",
      "0.4744318\n",
      "0.4769022\n",
      "0.47851562\n",
      "0.480625\n",
      "0.48497596\n",
      "0.4866898\n",
      "0.48493305\n",
      "0.4849138\n",
      "0.48645833\n",
      "0.4873992\n",
      "0.4868164\n",
      "0.48295453\n",
      "0.4834559\n",
      "0.4830357\n",
      "0.4826389\n",
      "0.4873311\n",
      "0.48766446\n",
      "0.4903846\n",
      "0.49257812\n",
      "0.49314025\n",
      "0.49293154\n",
      "0.49127907\n",
      "0.48863637\n",
      "0.4888889\n",
      "0.48913044\n",
      "0.4900266\n",
      "0.49153647\n",
      "0.49139032\n",
      "0.491875\n",
      "0.49111518\n",
      "0.4903846\n",
      "0.4896816\n",
      "0.48813656\n",
      "0.48835227\n",
      "0.48604912\n",
      "0.48601973\n",
      "0.48572198\n",
      "0.48569915\n",
      "0.48828125\n",
      "0.48796105\n",
      "0.48865926\n",
      "0.48809522\n",
      "0.4885254\n",
      "0.4875\n",
      "0.4876894\n",
      "0.48810634\n",
      "0.48851103\n",
      "0.48867753\n",
      "0.48950893\n",
      "0.4881162\n",
      "0.48828125\n",
      "0.4884418\n",
      "0.4881757\n",
      "0.48875\n",
      "0.48889804\n",
      "0.48843345\n",
      "0.48758012\n",
      "0.48674843\n",
      "0.48789063\n",
      "0.48688272\n",
      "0.48647103\n",
      "0.48625752\n",
      "0.48493305\n",
      "0.48547795\n",
      "0.48619187\n",
      "0.48742816\n",
      "0.48686078\n",
      "0.4866573\n",
      "0.4859375\n",
      "0.48677886\n",
      "0.48692256\n",
      "0.4877352\n",
      "0.48819813\n",
      "0.48832238\n",
      "0.48828125\n",
      "0.48872423\n",
      "0.4885204\n",
      "0.4872159\n",
      "0.48703125\n",
      "0.48654085\n",
      "0.4857537\n",
      "0.48649877\n",
      "0.48647836\n",
      "0.48616073\n",
      "0.48584905\n",
      "0.48554322\n",
      "0.4855324\n",
      "0.48566514\n",
      "0.4852273\n",
      "0.4852196\n",
      "0.48507255\n",
      "0.48520464\n",
      "0.4850603\n",
      "0.48410326\n",
      "0.4845097\n",
      "0.48544338\n",
      "0.4850371\n",
      "0.48555672\n",
      "0.48541668\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-eaa1911f9b3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mevaluate_with_k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mevaluate_with_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Projects\\csi5138-project\\experiment.py\u001b[0m in \u001b[0;36mevaluate_with_k\u001b[1;34m(model, test_dataset, adv_model)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         loss_t, k, best_loss_t, best_adv_k = find_adv_k(x, y, model,\n\u001b[1;32m--> 268\u001b[1;33m                 adv_model, adv_optimizer, epochs=30,random_k=False)\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_k\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_adv_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Projects\\csi5138-project\\experiment.py\u001b[0m in \u001b[0;36mfind_adv_k\u001b[1;34m(x, y, transformer, adv_model, adv_optimizer, epochs, random_k)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mbest_adv_k\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0madv_k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0madv_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madv_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0m_ShapesFullySpecifiedAndEqual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m       grad.dtype in (dtypes.int32, dtypes.float32)):\n\u001b[1;32m-> 1177\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m   \u001b[1;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" vs. \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\csi5138-project\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6683\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   6684\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Mul\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6685\u001b[1;33m         name, _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[0;32m   6686\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6687\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from experiment import evaluate_with_k\n",
    "evaluate_with_k(transformer, test_dataset, adv_model=None)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "transformer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0749ff95f94e407392f752a6f8dd62b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4d0d42c1651c46408a43d56e7f8dc221",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8ffe582b3c2a4c6b8dd620ad8cead256",
       "value": 1
      }
     },
     "0de9999544a342f5aeb6e14c68791cfb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "131696bfd4c94d148794eb68dede69d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "148d886fdc1c4ae2a59d3fad5c5f9ff0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0749ff95f94e407392f752a6f8dd62b7",
        "IPY_MODEL_7aa52080f14d4093b1c86bca6a50c798"
       ],
       "layout": "IPY_MODEL_ccfc125994594a4990bd67da3133e69b"
      }
     },
     "15242d55f5334583bf0c3985739e4c56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fac566e920704aa083922ac4c13a8c09",
        "IPY_MODEL_31664e7a64844e3aa3f4ec9d8ad655ff"
       ],
       "layout": "IPY_MODEL_bd905e190c304468a39f39a1e27d8281"
      }
     },
     "1723674f1abe4631bb872d905d01e82e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1814775d294a4820a03457a2c405a595": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1a4fa0e37f5b49aebec184114859f06c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d5dc258777541fc8bf74044d92985ab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2a14245cfada4d6b97484b39bd2a6472": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "2b8641548614466383e6bbdeac11cfe0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "Shuffling...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a8e38e4f78684fcdbe3a573eaf1a625b",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8e374429a8a64ce789d7f6f7526bb620",
       "value": 1
      }
     },
     "2bb1cc7b37bf428d87b9d9688d6f813f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2d2f0e419760467fad32c910f8543c61": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f54285fe4e64834951ca8417fcedc8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "31664e7a64844e3aa3f4ec9d8ad655ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0de9999544a342f5aeb6e14c68791cfb",
       "placeholder": "",
       "style": "IPY_MODEL_72eb69fe207e47a999ea74a846b2ab98",
       "value": " 1/1 [00:07&lt;00:00,  7.82s/ url]"
      }
     },
     "3933e8871ab44e35bb35075e980e1249": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "Writing...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2bb1cc7b37bf428d87b9d9688d6f813f",
       "max": 51785,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a05887713ecb45588d294bea27d52be5",
       "value": 51785
      }
     },
     "39b7c8a134f5436089d1ae1c7a46010a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "Reading...: ",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b640b07243f146f19aff313a2ba9dbcc",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bbab9298be594e06b90650d58096ce51",
       "value": 1
      }
     },
     "3be45321f4d741dc98f82ba6304c32b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3cb908e54b1f4186a8cac2fb8c14b95c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Extraction completed...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_92c135fc4bf740bfa6d13b800440fe6c",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e07e95d6001f48b2a0c7e8e9a6b09c75",
       "value": 1
      }
     },
     "489996844cb240e4998b1a9df036e349": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ab318abad5364193b164d8130e65ba0f",
       "placeholder": "",
       "style": "IPY_MODEL_a44864ec5632458685cb9c63a655916b",
       "value": " 1803/? [00:00&lt;00:00, 74200.14 examples/s]"
      }
     },
     "4d0d42c1651c46408a43d56e7f8dc221": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "56c69776735c4456aa7f495020d61665": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "58827d88d0a645a5a54893a6af1ec88a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5978fed625cd41428a1c74e967422719": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "59c6457692054e87a313076bdfe97828": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "5a2f4fd48e244d588f86aa336619d76d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5a3c51337f4c4481a8f9ee1310e59415": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "5b254626037f4647bf930c133fed2404": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2b8641548614466383e6bbdeac11cfe0",
        "IPY_MODEL_9d51f1d6e8b84142984e288722e91418"
       ],
       "layout": "IPY_MODEL_acdda6601e5041968ee300f67bbad439"
      }
     },
     "5ee903f30e314ccf9bc1e5dbd3f39d63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_622d429ca7db459cbf06904f34e22365",
        "IPY_MODEL_96c9bcec854f450fbb314075933d61dc"
       ],
       "layout": "IPY_MODEL_c82c9a3dc280452491c961e7a67bf063"
      }
     },
     "622d429ca7db459cbf06904f34e22365": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "Shuffling...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6c9cffc1c9f34c4d8fe37306bb43d0e4",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5a3c51337f4c4481a8f9ee1310e59415",
       "value": 1
      }
     },
     "67415724c3a54a43b1ae7745bbcbb198": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6aad7f146bd24fd290da6e7508e086a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "6b79a6e9ce6b49f1ae887edd07210cc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "Reading...: ",
       "description_tooltip": null,
       "layout": "IPY_MODEL_76caf69e773e4745b018a974585a7df0",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_59c6457692054e87a313076bdfe97828",
       "value": 1
      }
     },
     "6c3777c8b13144f29d155bcf8d2475e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Dl Size...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1814775d294a4820a03457a2c405a595",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d8f2a6b311974782b348908acea3c817",
       "value": 1
      }
     },
     "6c9cffc1c9f34c4d8fe37306bb43d0e4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6caea86519aa4fd9a3a2ed90ea9d119d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6ce52ca8bf7c4f3fa8e3e5e1d3be4f3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6cfed32842fd45de936ec91dfdda8d13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "71e937b4526d4f0f8fd3c6ff2593fdca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "729c02f630494513b109e9b9dda657db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fbc558ada6da47a0a656eb85c2dfb19c",
        "IPY_MODEL_c171588f4ed14b62ae94650036a45986"
       ],
       "layout": "IPY_MODEL_67415724c3a54a43b1ae7745bbcbb198"
      }
     },
     "72eb69fe207e47a999ea74a846b2ab98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7361b8fd1cb24d2b87de92b21ebb914b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_be4332061a9c4726a1def95032aa97ad",
       "placeholder": "",
       "style": "IPY_MODEL_9574252710814c71b3cbb56cf2c65cf2",
       "value": " 51785/? [00:00&lt;00:00, 484155.84 examples/s]"
      }
     },
     "74ae5400403b4c30a09e1664def437ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "76caf69e773e4745b018a974585a7df0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "79b7d550637246c481c90707a5852140": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ee21f8db80c4484d878715652bd4b654",
        "IPY_MODEL_a864658d387b47a8bb48c13e07c16abd"
       ],
       "layout": "IPY_MODEL_56c69776735c4456aa7f495020d61665"
      }
     },
     "7aa52080f14d4093b1c86bca6a50c798": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fcfabd9c5aad4c1c9279d7e3c443269f",
       "placeholder": "",
       "style": "IPY_MODEL_1723674f1abe4631bb872d905d01e82e",
       "value": " 1193/? [00:00&lt;00:00, 6148.20 examples/s]"
      }
     },
     "7dbce8926d6c4b43a2acebe7970813ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3be45321f4d741dc98f82ba6304c32b8",
       "placeholder": "",
       "style": "IPY_MODEL_131696bfd4c94d148794eb68dede69d5",
       "value": " 1803/? [00:00&lt;00:00, 6427.76 examples/s]"
      }
     },
     "80135a37e96d4438a5cb29de86fb8351": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_39b7c8a134f5436089d1ae1c7a46010a",
        "IPY_MODEL_7361b8fd1cb24d2b87de92b21ebb914b"
       ],
       "layout": "IPY_MODEL_5978fed625cd41428a1c74e967422719"
      }
     },
     "82038a7d31a34a529a35991012929f5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "Writing...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2d2f0e419760467fad32c910f8543c61",
       "max": 1193,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6aad7f146bd24fd290da6e7508e086a5",
       "value": 1193
      }
     },
     "84ba749dd80149dba5c32ccc855f8cef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "85078def24364829b56fe5e49874fc15": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8731841a56e647178eb5b238c1903510": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a77d338edce84bcba509cbaa2ba465a1",
        "IPY_MODEL_489996844cb240e4998b1a9df036e349"
       ],
       "layout": "IPY_MODEL_89789743e7594550aaec30885a4fb2d0"
      }
     },
     "89789743e7594550aaec30885a4fb2d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a12e1c3d0224c6f8ca7235b16a96df6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ba6c2112b484325bd3572f8ad544789": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6c3777c8b13144f29d155bcf8d2475e9",
        "IPY_MODEL_d919ec5b8e484cb4a598890c34c4a469"
       ],
       "layout": "IPY_MODEL_6cfed32842fd45de936ec91dfdda8d13"
      }
     },
     "8e1806cc299f490796fa298905b76649": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e374429a8a64ce789d7f6f7526bb620": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "8ebe355cdd2443a69ce2975f3fbbd87d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8ffe582b3c2a4c6b8dd620ad8cead256": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "92c135fc4bf740bfa6d13b800440fe6c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "934964ebecb34b1c9d12ab1b2347e1ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_df8c1015e0c2444dbbcf9b47f683f162",
        "IPY_MODEL_bdc86b3800ef42abbe0fb9ad51761778"
       ],
       "layout": "IPY_MODEL_85078def24364829b56fe5e49874fc15"
      }
     },
     "9574252710814c71b3cbb56cf2c65cf2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "96c9bcec854f450fbb314075933d61dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bcb0ce91159a4c7283eebab52ae76ac0",
       "placeholder": "",
       "style": "IPY_MODEL_5a2f4fd48e244d588f86aa336619d76d",
       "value": " 1/1 [00:00&lt;00:00,  3.02 shard/s]"
      }
     },
     "9b78b673515944b4bba14a2b27947839": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9c789fe4643e43d1a7286d5a0584af29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9d51f1d6e8b84142984e288722e91418": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_58827d88d0a645a5a54893a6af1ec88a",
       "placeholder": "",
       "style": "IPY_MODEL_a163295458634e3b96d51da30cf10fe6",
       "value": " 1/1 [00:00&lt;00:00, 16.11 shard/s]"
      }
     },
     "9ef47f376f8542ca9346148d7241fc71": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "9fbac593e9d74538a266dde790870d0e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8a12e1c3d0224c6f8ca7235b16a96df6",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bb3dafc7078046b993a3ac0f566c6722",
       "value": 1
      }
     },
     "a05887713ecb45588d294bea27d52be5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "a163295458634e3b96d51da30cf10fe6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a324c8fa440140249976a0bdc677ab51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1d5dc258777541fc8bf74044d92985ab",
       "placeholder": "",
       "style": "IPY_MODEL_fa9ca14603e74963b5e7db0300f0c359",
       "value": " 1/1 [00:07&lt;00:00,  7.78s/ file]"
      }
     },
     "a44864ec5632458685cb9c63a655916b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a621c43cc8914670b16663e933b82967": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9fbac593e9d74538a266dde790870d0e",
        "IPY_MODEL_7dbce8926d6c4b43a2acebe7970813ae"
       ],
       "layout": "IPY_MODEL_6caea86519aa4fd9a3a2ed90ea9d119d"
      }
     },
     "a77d338edce84bcba509cbaa2ba465a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "Reading...: ",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e508cf6515074f78bc2db36e153b6500",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_deb538c6c51141a694e60a559b1e4dda",
       "value": 1
      }
     },
     "a864658d387b47a8bb48c13e07c16abd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b57bfd2a6b7f4005b96459ea72c39138",
       "placeholder": "",
       "style": "IPY_MODEL_e6700935b8a84b0da50650e34b43fbee",
       "value": " 1803/1803 [00:00&lt;00:00, 89253.15 examples/s]"
      }
     },
     "a8e38e4f78684fcdbe3a573eaf1a625b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa70f3b49687450fb42cc3b96151bdd0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b19a9b4b7b4744018b801b3daaa81665",
       "placeholder": "",
       "style": "IPY_MODEL_d2bdb1b6ac484331b2c72262b9adfeb2",
       "value": " 1193/1193 [00:00&lt;00:00, 66844.85 examples/s]"
      }
     },
     "ab318abad5364193b164d8130e65ba0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "acdda6601e5041968ee300f67bbad439": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b19a9b4b7b4744018b801b3daaa81665": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b4acb61c122547ba9934437bbb36f557": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b57bfd2a6b7f4005b96459ea72c39138": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b640b07243f146f19aff313a2ba9dbcc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bb3dafc7078046b993a3ac0f566c6722": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "bbab9298be594e06b90650d58096ce51": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "bca4a30700314b439bda95feaf18a27d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bcb0ce91159a4c7283eebab52ae76ac0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bd905e190c304468a39f39a1e27d8281": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bdc86b3800ef42abbe0fb9ad51761778": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d32a12762b784ff38806d9a098ddebbe",
       "placeholder": "",
       "style": "IPY_MODEL_fb8fbc6ad6f7447dbe59da1f55f74bad",
       "value": " 1/1 [00:00&lt;00:00, 17.14 shard/s]"
      }
     },
     "be4332061a9c4726a1def95032aa97ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c171588f4ed14b62ae94650036a45986": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_eda4544e884444d1b9e2392b4d09307a",
       "placeholder": "",
       "style": "IPY_MODEL_6ce52ca8bf7c4f3fa8e3e5e1d3be4f3d",
       "value": " 51785/? [00:07&lt;00:00, 7601.16 examples/s]"
      }
     },
     "c498f668b9ed4672ab9501ddfcaa7a19": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bca4a30700314b439bda95feaf18a27d",
       "placeholder": "",
       "style": "IPY_MODEL_74ae5400403b4c30a09e1664def437ee",
       "value": " 1193/? [00:00&lt;00:00, 52883.16 examples/s]"
      }
     },
     "c709dfdf94e142b5929cce71dac241d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "c82c9a3dc280452491c961e7a67bf063": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ccfc125994594a4990bd67da3133e69b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ce2b1e4e8b4345279380889757374fa8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d2bdb1b6ac484331b2c72262b9adfeb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d32a12762b784ff38806d9a098ddebbe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8ba9b798e2f4b349a30a3a19ed42aeb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_82038a7d31a34a529a35991012929f5f",
        "IPY_MODEL_aa70f3b49687450fb42cc3b96151bdd0"
       ],
       "layout": "IPY_MODEL_84ba749dd80149dba5c32ccc855f8cef"
      }
     },
     "d8f2a6b311974782b348908acea3c817": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "d919ec5b8e484cb4a598890c34c4a469": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_71e937b4526d4f0f8fd3c6ff2593fdca",
       "placeholder": "",
       "style": "IPY_MODEL_2f54285fe4e64834951ca8417fcedc8f",
       "value": " 124/124 [00:07&lt;00:00, 15.90 MiB/s]"
      }
     },
     "dc21853b661e4ae2b498f6319efb1f08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3933e8871ab44e35bb35075e980e1249",
        "IPY_MODEL_dfdf5ddfde4d48559ca0894431936d53"
       ],
       "layout": "IPY_MODEL_f52f1747360948fd8af60e857c646349"
      }
     },
     "ddbcf6bf818d4e8fae4c23ba730cbab6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "deb538c6c51141a694e60a559b1e4dda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "df8c1015e0c2444dbbcf9b47f683f162": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "Shuffling...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8ebe355cdd2443a69ce2975f3fbbd87d",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2a14245cfada4d6b97484b39bd2a6472",
       "value": 1
      }
     },
     "dfdf5ddfde4d48559ca0894431936d53": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8e1806cc299f490796fa298905b76649",
       "placeholder": "",
       "style": "IPY_MODEL_eef0efbe91ae4a75bad0b21f311ea7c9",
       "value": " 51785/51785 [00:00&lt;00:00, 283142.84 examples/s]"
      }
     },
     "e07e95d6001f48b2a0c7e8e9a6b09c75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "e508cf6515074f78bc2db36e153b6500": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e583e3ce21294988a13ffb02a8ac4a3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3cb908e54b1f4186a8cac2fb8c14b95c",
        "IPY_MODEL_a324c8fa440140249976a0bdc677ab51"
       ],
       "layout": "IPY_MODEL_ce2b1e4e8b4345279380889757374fa8"
      }
     },
     "e6700935b8a84b0da50650e34b43fbee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "eda4544e884444d1b9e2392b4d09307a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee21f8db80c4484d878715652bd4b654": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "Writing...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9b78b673515944b4bba14a2b27947839",
       "max": 1803,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c709dfdf94e142b5929cce71dac241d8",
       "value": 1803
      }
     },
     "eef0efbe91ae4a75bad0b21f311ea7c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f3485591da6d438097d02d5f9815306b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6b79a6e9ce6b49f1ae887edd07210cc1",
        "IPY_MODEL_c498f668b9ed4672ab9501ddfcaa7a19"
       ],
       "layout": "IPY_MODEL_1a4fa0e37f5b49aebec184114859f06c"
      }
     },
     "f52f1747360948fd8af60e857c646349": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa9ca14603e74963b5e7db0300f0c359": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fac566e920704aa083922ac4c13a8c09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Dl Completed...: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b4acb61c122547ba9934437bbb36f557",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9ef47f376f8542ca9346148d7241fc71",
       "value": 1
      }
     },
     "fb8fbc6ad6f7447dbe59da1f55f74bad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fbc558ada6da47a0a656eb85c2dfb19c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "info",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9c789fe4643e43d1a7286d5a0584af29",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ddbcf6bf818d4e8fae4c23ba730cbab6",
       "value": 1
      }
     },
     "fcfabd9c5aad4c1c9279d7e3c443269f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
